Advanced Project: AI-Powered Data Pipeline with Full CI/CD (Without AWS)
Project Overview

This project builds an AI-powered, fully automated CI/CD pipeline using:
âœ… VS Code (development)
âœ… Python (data processing & ML model)
âœ… GitHub (version control)
âœ… Jenkins (automation)
âœ… Docker (containerization)
âœ… PostgreSQL (database)
âœ… FastAPI + Nginx (real-time inference with load balancing)
âœ… Terraform (infrastructure automation, now for local deployment)

ðŸš€ What You'll Build:

    Develop & train an ML model using PostgreSQL data.

    Automate training, packaging, and deployment using Jenkins & Docker.

    Deploy FastAPI for real-time inference behind Nginx as a reverse proxy.

    Use Terraform to automate infrastructure on a local Linux server.

Step 1: Setting Up the Development Environment
1.1 Install Dependencies

Ensure you have installed:

    VS Code

    Python 3.9+

    Git & GitHub

    Jenkins

    Docker & Docker Compose

    Terraform

    PostgreSQL

1.2 Create Project Structure

mkdir ai_data_pipeline && cd ai_data_pipeline
python -m venv venv
source venv/bin/activate
pip install pandas psycopg2 matplotlib scikit-learn joblib fastapi uvicorn

Step 2: Data Processing & ML Model
2.1 Create db_connector.py

Handles database connections.

import psycopg2

def get_db_connection():
    return psycopg2.connect(
        dbname="mbarreras_db",
        user="your_user",
        password="your_password",
        host="localhost",
        port="5432"
    )

2.2 Create train_model.py

Trains a logistic regression model for churn prediction.

import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from db_connector import get_db_connection

def train_model():
    conn = get_db_connection()
    df = pd.read_sql("SELECT * FROM customer", conn)
    conn.close()
    
    X = df[['age', 'income', 'account_balance']]
    y = df['churn']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = LogisticRegression()
    model.fit(X_train, y_train)
    
    joblib.dump(model, "model.pkl")
    print("Model trained and saved.")

if __name__ == "__main__":
    train_model()

Step 3: Deploy FastAPI Locally with Nginx
3.1 Create app.py

Deploys FastAPI for real-time prediction.

from fastapi import FastAPI
import joblib
import numpy as np

app = FastAPI()
model = joblib.load("model.pkl")

@app.post("/predict")
def predict(data: dict):
    features = np.array([data['age'], data['income'], data['account_balance']]).reshape(1, -1)
    prediction = model.predict(features)
    return {"churn_probability": prediction.tolist()}

3.2 Create Dockerfile

FROM python:3.9
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]

3.3 Create nginx.conf

Configures Nginx as a reverse proxy for FastAPI.

server {
    listen 80;
    
    location / {
        proxy_pass http://fastapi:8000;
    }
}

3.4 Create docker-compose.yml

Defines FastAPI and Nginx services.

version: '3.8'

services:
  fastapi:
    build: .
    ports:
      - "8000:8000"
  
  nginx:
    image: nginx:latest
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "80:80"
    depends_on:
      - fastapi

Step 4: Automate with Terraform (For Local Deployment)
4.1 Install & Configure Terraform

terraform init

4.2 Create main.tf

Defines the local server infrastructure.

provider "local" {}

resource "local_file" "nginx_config" {
  filename = "nginx.conf"
  content  = <<EOT
server {
    listen 80;
    
    location / {
        proxy_pass http://localhost:8000;
    }
}
EOT
}

resource "null_resource" "start_containers" {
  provisioner "local-exec" {
    command = "docker-compose up -d"
  }
}

4.3 Deploy Infrastructure

terraform apply -auto-approve

Step 5: Automate with Jenkins
5.1 Create Jenkinsfile

pipeline {
    agent any
    stages {
        stage('Checkout Code') {
            steps {
                git 'https://github.com/yourusername/ai_data_pipeline.git'
            }
        }
        stage('Train Model') {
            steps {
                sh 'python train_model.py'
            }
        }
        stage('Build Docker Image') {
            steps {
                sh 'docker build -t ai-pipeline .'
            }
        }
        stage('Run Docker Compose') {
            steps {
                sh 'docker-compose up -d'
            }
        }
    }
}

Final Workflow

    Develop & Train Model in VS Code

    Push Code to GitHub (Triggers Jenkins)

    Jenkins Automates:

        Pulls Code

        Trains Model

        Builds Docker Image

        Deploys FastAPI & Nginx Locally

    FastAPI Exposes an API for real-time predictions

Why is This Challenging?

âœ… Full CI/CD Pipeline (GitHub â†’ Jenkins â†’ Docker â†’ Local Server)
âœ… AI-Powered Prediction System
âœ… Infrastructure as Code (Terraform)
âœ… ML Model Training & Deployment
âœ… Load Balancing & Reverse Proxy (Nginx)

This is an advanced production-grade project without relying on AWS! Would you like additional customizations? ðŸš€
